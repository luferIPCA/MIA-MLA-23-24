{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1kf9qddTM4D"
      },
      "source": [
        "# Masters' in Applied Artificial Intelligence\n",
        "## Machine Learning Algorithms Course\n",
        "\n",
        "Notebooks for the MLA course\n",
        "\n",
        "by [*lufer*](mailto:lufer@ipca.pt)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yc6mD0jVeWN"
      },
      "source": [
        "# ML Modelling - Part VII - Improving a Model\n",
        "\\\n",
        "**Contents**:\n",
        "\n",
        "1.  **Model Improvement**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook explores the reqyirements adn processes to improve a ML model."
      ],
      "metadata": {
        "id": "iAEg0vfdoiFr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP-NymupVL02"
      },
      "source": [
        "# Environment preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3Rm857IVoPe"
      },
      "source": [
        "**Importing necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vA1MzNI4TU_q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "#import libraries for trainning\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "print(f\"Last updated: {datetime.datetime.now()}\")"
      ],
      "metadata": {
        "id": "wuXym8xInfIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDLxcgMwJEYA"
      },
      "source": [
        "**Mounting Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxFY0ypTJJK9"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# it will ask for your google drive credentiaals\n",
        "drive.mount('/content/gDrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LqOOL0uDgaqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - What ML Algorithms are there?"
      ],
      "metadata": {
        "id": "Ceu0N5uOmj2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose a Machine Learning algortihms depend of many factors, such as the size of the datatset, the type of the data in it, the goal of the model, and others."
      ],
      "metadata": {
        "id": "7jmjZq7lp3Zq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sklearn offers a graphical algorith that facilicates this selection.\n",
        "\n",
        "![picture](https://scikit-learn.org/stable/_static/ml_map.png)"
      ],
      "metadata": {
        "id": "_X9DskdMqdnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Improving a Model\n",
        "\n",
        "\n",
        "RandomForest is one emsamble model that has very good performances."
      ],
      "metadata": {
        "id": "W-9uny3ilsmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**\n",
        "\\\n",
        "The first predictions for the first model, represents the initial results! Can it be improved?\n",
        "\n",
        "\\\n",
        "The improving can be done on data, model, parameters:\n",
        "\n",
        "* From the data perspective:\n",
        "\n",
        "> - Can the data be improved?\n",
        "> - Could we collect more data (the more data, the better, usually!)\n",
        "\n",
        "* From the model perspective\n",
        "\n",
        "> - Is there a better model?\n",
        "> - Can de current model be improved? (exploring hyperparameters)\n",
        "\n",
        "\n",
        "*Hyperparameters versus Parameters*\n",
        "\n",
        "* *Parameters* = correspond to patterns that model found in data\n",
        "* *Hyperparameters* = are model settings that we can try to adjust  \n",
        "\n",
        "\\\n",
        "How to manipulate Hyperparameters:\n",
        "1. Manually\n",
        "2. Randomly with [RandomSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
        "3. Exhaustively with [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n"
      ],
      "metadata": {
        "id": "Yhz9QWwupiCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Download Dataset*\n",
        "\n"
      ],
      "metadata": {
        "id": "VFv4l95wtjKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing a real world dataset preparaed for Regression\n",
        "\n",
        "filePath='/content/gDrive/MyDrive/MIA/ColabNotebooks/Datasets/'\n",
        "hd = pd.read_csv(filePath+\"heart-disease.csv\")\n",
        "pd.set_option(\"display.precision\", 2)\n",
        "#answer: a dictionary"
      ],
      "metadata": {
        "id": "APRXNHdxoiV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hd.head()"
      ],
      "metadata": {
        "id": "tJygnSC4n0iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Model"
      ],
      "metadata": {
        "id": "prz6hA8NqHa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#importing metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
      ],
      "metadata": {
        "id": "uO2pLsDp35e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Data for training - *Features* - (X) and Ground Truth Values - *Labels* (y)**"
      ],
      "metadata": {
        "id": "Vh_z_SS2DHtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create X (all the feature columns)\n",
        "X = hd.drop(\"target\", axis=1)\n",
        "\n",
        "# Create y (the target column)\n",
        "y = hd[\"target\"]\n",
        "\n",
        "# Check the head of the features DataFrame\n",
        "X.head()"
      ],
      "metadata": {
        "id": "foK6ChpJDD-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the head and the value counts of the labels\n",
        "y.head()"
      ],
      "metadata": {
        "id": "-k1DeLrBoEMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.value_counts()"
      ],
      "metadata": {
        "id": "QAbBd2QloFsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Suppose modeling without split data (Don't do this!)!**"
      ],
      "metadata": {
        "id": "VKKJuDFRgIcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# fit the model on one set of data\n",
        "rf.fit(X, y)\n",
        "\n",
        "# evaluate the model on the second set of data\n",
        "y_model = rf.predict(X)\n",
        "\n",
        "accuracy_score(y, y_model)"
      ],
      "metadata": {
        "id": "lx2UzpaCgHWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split dataset**"
      ],
      "metadata": {
        "id": "t5sTWBbHrQAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.3) # by default train_test_split uses 25% of the data for the test set\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "RBQsuIGQrSYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the sizes of the splits\n",
        "print(f\"Training data: {len(X_train)} samples, {len(y_train)} labels\")\n",
        "print(f\"Validation data: {len(X_test)} samples, {len(y_test)} labels\")"
      ],
      "metadata": {
        "id": "xtHiEUwzaT_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get the Model**"
      ],
      "metadata": {
        "id": "u8Yo_7lerpM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we're working on a classification problem, we'll use a RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier()\n"
      ],
      "metadata": {
        "id": "6Qkn0OiQrCPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the Model**"
      ],
      "metadata": {
        "id": "0BjccztAd0Qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "lYqMeYgId7Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.head()"
      ],
      "metadata": {
        "id": "mX9wRRbha8zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predicting**"
      ],
      "metadata": {
        "id": "SGaoHpM9rs0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the model to make a prediction on the test data (further evaluation)\n",
        "y_preds = rf.predict(X_test)\n",
        "y_preds"
      ],
      "metadata": {
        "id": "mq-hvHQBrvHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate**\n",
        "\n",
        "\\\n",
        "*Remember:*\n",
        "\n",
        "\n",
        "Each model  has a built-in `score()` method.\n",
        "\n",
        "- The `score()` method compares how well the model was able to learn the patterns between the features and labels.\n",
        "\n",
        "- The `score()` method uses a standard evaluation metric to measure the model's results.\n",
        "- The `score()`represents the `accuracy` of the model\n"
      ],
      "metadata": {
        "id": "Ra8EM7UUsPkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the training set.\n",
        "train_acc = rf.score(X_train, y_train)\n",
        "print(f\"The model's accuracy on the training dataset is: {train_acc*100}%\")\n",
        "#perfect! why?"
      ],
      "metadata": {
        "id": "pokzpJIRsR6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_acc = rf.score(X_test, y_test)\n",
        "print(f\"The model's accuracy on the testing dataset is: {test_acc*100:.2f}%\")\n",
        "#worst than previous prediction. Why?"
      ],
      "metadata": {
        "id": "5AK4Ep7Issz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Other evaluation methods;\n",
        "- `classification_report(y_true, y_pred)` - Builds a text report showing various classification metrics such as `precision`, `recall` and `F1-score`.\n",
        "- `confusion_matrix(y_true, y_pred)` - Create a confusion matrix to compare predictions to truth labels.\n",
        "- `accuracy_score(y_true, y_pred)`- Find the accuracy score (the default metric) for a classifier.\n",
        "\n",
        "\\\n",
        "**Note:** All metrics have the following in common: they compare a model's predictions (y_pred) to truth labels (y_true)."
      ],
      "metadata": {
        "id": "Dohb78rGtHcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Create a classification report\n",
        "print(classification_report(y_test, y_preds))"
      ],
      "metadata": {
        "id": "cAc9_VzFtoWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a confusion matrix\n",
        "conf_mat = confusion_matrix(y_test, y_preds)\n",
        "conf_mat"
      ],
      "metadata": {
        "id": "K1R7L8jstxOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the accuracy score (same as the score() method for classifiers)\n",
        "accuracy_score(y_test, y_preds), rf.score(X_test,y_test)"
      ],
      "metadata": {
        "id": "NLxkPa_Yt036"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JP1kCTw1h9se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a function to help metrics calculation**"
      ],
      "metadata": {
        "id": "w9kTvRI594wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#show metrics\n",
        "def evaluate_preds(y_true, y_preds):\n",
        "    \"\"\"\n",
        "    Performs evaluation comparison on y_true labels vs. y_pred labels\n",
        "    \"\"\"\n",
        "    accuracy = accuracy_score(y_true, y_preds)\n",
        "    precision = precision_score(y_true, y_preds)\n",
        "    recall = recall_score(y_true, y_preds)\n",
        "    f1 = f1_score(y_true, y_preds)\n",
        "    metric_dict = {\"accuracy\": round(accuracy, 2),\n",
        "                   \"precision\": round(precision, 2),\n",
        "                   \"recall\": round(recall, 2),\n",
        "                   \"f1\": round(f1, 2)}\n",
        "    print(f\"Acc: {accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1 score: {f1:.2f}\")\n",
        "\n",
        "    return metric_dict"
      ],
      "metadata": {
        "id": "JHGok5e5jBmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs_metrics = evaluate_preds(y_test, y_preds)\n",
        "#bs_metrics"
      ],
      "metadata": {
        "id": "Dc2Cs-v5RMaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improving processes\n",
        "\n",
        "We can try changing each hyperparamater (hereafter abbreviated by parameter) value.\n",
        "\\\n",
        "For instance, we can try change `n_estimators` or `random-state`.\n",
        "\n",
        "\n",
        "**Notes**:\n",
        "- All experiments with different parameters should be cross-validated.\n",
        "  - **Attention:** Beware of cross-validation for time series problems (as for time series, you don't want to mix samples from the future with samples from the past).\n",
        "- All experiments will be applied over three datasets: Training, Validation and Testing.\n",
        "\n",
        "\\\n",
        "Has we saw above, there are three ways to manipulate Hyperparameters:\n",
        "\n",
        "- Manually\n",
        "- Randomly with RandomSearchCV\n",
        "- Exhaustively with GridSearchCV\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "- `RandomizedSearchCV` implements a “fit” and a “score” method. It also implements “score_samples”, “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.\n",
        "\n",
        "- `GridSearchCV`, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions.\n",
        "\n",
        "\n",
        "[see more about in... ](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n"
      ],
      "metadata": {
        "id": "ZZ6C9qIdBaV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Check existing Hyperparameters**"
      ],
      "metadata": {
        "id": "6pY2nSLK2Smk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#craate a new instance\n",
        "#rf = RandomForestClassifier()\n",
        "rf.get_params()\n",
        "#Hyperparameters"
      ],
      "metadata": {
        "id": "lL8HmCOzyWgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "There are several (hyper)parameters can be explored. It is also possible to see the default values for each (hyper)parameter.\n",
        "\\\n"
      ],
      "metadata": {
        "id": "zEHWSiNN4S3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explore Hyperparameters manually**\n",
        "\n",
        "\n",
        "Hyperparameter tuning introduces a third set, a validation set.\n",
        "\n",
        "So the process becomes:\n",
        "1. Train a model on the training data.\n",
        "2. (Try to) improve the model's hyperparameters on the validation set.\n",
        "3. Evaluate the model on the test set."
      ],
      "metadata": {
        "id": "o8Sb05ZVwoDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating the Validation Set**"
      ],
      "metadata": {
        "id": "pV-rp3wMChsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Until now we worked with training and test datasets. We train a model on a training set and evaluate it on a test dataset.\n",
        "\n",
        "\\\n",
        "For that we split data into Train and Test Data, using the `train_test_split()`function.\n",
        "\n",
        "\\\n",
        "Now we repeat the process to split the Test Set in two new sets: Validation and Test set.\n",
        "\n",
        "\n",
        "```\n",
        "X = Train + Validation + Test\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "XHCv6h_mCp20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Set the seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# Split into X (features) & y (labels)\n",
        "X = hd.drop(\"target\", axis=1)\n",
        "y = hd[\"target\"]\n",
        "\n",
        "# Training and test split (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)\n",
        "\n",
        "# Create validation and test split by spliting testing data in half (30% test -> 15% validation, 15% test)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5)\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions in X_valid\n",
        "y_preds = clf.predict(X_valid)\n",
        "\n",
        "# Evaluate the classifier\n",
        "baseline_metrics = evaluate_preds(y_valid, y_preds)\n",
        "baseline_metrics"
      ],
      "metadata": {
        "id": "8Wu4t3TrZnt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the sizes of the splits\n",
        "print(f\"Training data: {len(X_train)} samples, {len(y_train)} labels\")\n",
        "print(f\"Validation data: {len(X_valid)} samples, {len(y_valid)} labels\")\n",
        "print(f\"Testing data: {len(X_test)} samples, {len(y_test)} labels\")"
      ],
      "metadata": {
        "id": "MMMgf4jQI-hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remember**:\n",
        "\n",
        "On this improvement process, we use:\n",
        "\n",
        "- X_train data for Training\n",
        "- X_valid data for model testing\n",
        "- X_test for model Evaluation"
      ],
      "metadata": {
        "id": "2koEOcfHCyDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make predictions using `X_valid` data**"
      ],
      "metadata": {
        "id": "bqBjtAG4_ikA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#score\n",
        "a=rf.score(X_valid, y_valid)\n",
        "print(f\"Score: {a*100:.4f}%\")\n",
        "\n",
        "ac = accuracy_score(y_valid,y_preds)\n",
        "ac"
      ],
      "metadata": {
        "id": "RhsZysw8JY-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the model**\n",
        "\n",
        "The first evaluation is with default parameters"
      ],
      "metadata": {
        "id": "AL90g1M8KgwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the 1st classifier\n",
        "baseline_metrics = evaluate_preds(y_valid, y_preds)\n"
      ],
      "metadata": {
        "id": "VDAUXHFd_E2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lets try to improve the model, manually**\n",
        "\n",
        "Changing the hyperparameters `n_estimators=100` (default) to `n_estimators=200` and see if it improves on the validation set."
      ],
      "metadata": {
        "id": "Kok2daQVKs1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#new instance\n",
        "rf2 = RandomForestClassifier(n_estimators=200)\n",
        "rf2.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "6hlRCmDaLDqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make predictions wityh different hyperparameter\n",
        "y_preds2 = rf2.predict(X_valid)\n"
      ],
      "metadata": {
        "id": "kdO9AsERLKa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the 2nd classifier\n",
        "rf2_metrics = evaluate_preds(y_valid, y_preds2)"
      ],
      "metadata": {
        "id": "oLbXvRv7KwPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Without cross-validation**\n"
      ],
      "metadata": {
        "id": "L2kGl9FDxban"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try different numbers of estimators (trees)... (no cross-validation)\n",
        "np.random.seed(42)\n",
        "for i in range(100, 200, 10):\n",
        "    print(f\"->Trying model with {i} estimators...\")\n",
        "    model = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)\n",
        "    print(f\"Model accuracy on test set: {model.score(X_test, y_test) * 100:.2f}%\")\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "3RHY7Hglwv2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With cross-validation**\n",
        "\n",
        "\\\n",
        "*Remember:*\n",
        "\n",
        "[Cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html) is a way of making sure the results you're getting are more consistent across your training and test datasets (because it uses multiple versions of training and test sets) .\n",
        "\n",
        "- We can achieve this by setting `cross_val_score(X, y, cv=5)`.\n",
        "\n",
        "- `X` is the full feature set;\n",
        "- `y` is the full label set;\n",
        "- `cv` is the number of train and test splits\n",
        "\n",
        "\\\n",
        "`cross_val_score` will automatically create from the data (in this case, 5 different splits, this is known as 5-fold cross-validation).\n",
        "\n",
        "It is usually a better indicator of a quality model than a single split accuracy score."
      ],
      "metadata": {
        "id": "H1Ml0QBuxhN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# With cross-validation\n",
        "np.random.seed(42)\n",
        "for i in range(100, 200, 10):\n",
        "    print(f\"-> Trying model with [{i}] estimators...\")\n",
        "    model = RandomForestClassifier(n_estimators=i)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Measure the model score on a single train/test split\n",
        "    model_score = model.score(X_test, y_test)\n",
        "    print(f\"Model accuracy on single test set split: {model_score * 100:.2f}%\")\n",
        "\n",
        "    # Measure the mean cross-validation score across 5 different train and test splits\n",
        "    cross_val_mean = np.mean(cross_val_score(model, X, y, cv=5))\n",
        "    print(f\"5-fold cross-validation score: {cross_val_mean * 100:.2f}%\")\n",
        "\n",
        "    print(\"-------------------\")"
      ],
      "metadata": {
        "id": "fxfQupU-xjHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After getting the best performance (score with n_estimator=120, and cv=5) we can try adjust another hyperparameter."
      ],
      "metadata": {
        "id": "aka50uJaWoYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#try max_depth hyperparameter\n",
        "\n",
        "model = RandomForestClassifier(\n",
        "                                n_estimators=120,\n",
        "                                max_depth=10)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Measure the model score on a single train/test split\n",
        "model_score = model.score(X_test, y_test)\n",
        "print(f\"Model accuracy on single test set split: {model_score * 100:.2f}%\")\n",
        "\n",
        "# Measure the mean cross-validation score across 5 different train and test splits\n",
        "cross_val_mean = np.mean(cross_val_score(model, X, y, cv=5))\n",
        "print(f\"5-fold cross-validation score; n_estimator=160; max_depth=10: {cross_val_mean * 100:.2f}%\")\n",
        "#answer: worst or better?"
      ],
      "metadata": {
        "id": "oWoSTyXLfAdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And so on...continue \"trying and evaluate for each hyperparameter combination\"...a very hard work!"
      ],
      "metadata": {
        "id": "1KUKQlWjjdri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exploring *RandomizedSearchCV***\n",
        "\n",
        "\\\n",
        "First, it is necesssaary to create a *dictionary* of parameter distributions (collections of different values for specific hyperparamters) we'd like to analyse.\n",
        "\n",
        "```\n",
        "param_values = {\"hyperparameter_name\": [values_to_randomly_try]}\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "mdvTSYfD6N0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter grid for RandomizedSearchCV\n",
        "param_values = {\"n_estimators\": [10, 100, 200, 500, 1000, 1200],\n",
        "                       \"max_depth\": [None, 5, 10, 20, 30],\n",
        "                       \"max_features\": [\"sqrt\", \"log2\", None],\n",
        "                       \"min_samples_split\": [2, 4, 6, 8],\n",
        "                       \"min_samples_leaf\": [1, 2, 4, 8]}"
      ],
      "metadata": {
        "id": "FOauRvdVoShE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check\n",
        "param_values.values()"
      ],
      "metadata": {
        "id": "2n-NXPbNt8rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Curiosity\n",
        "# Count the total number of hyperparameter combinations to test\n",
        "total_combinations = np.prod([len(value) for value in param_values.values()])\n",
        "# 6 * 5 * 3 * 4 * 4 = 1440\n",
        "#if we consider cv=5 then it will be necessary 1440*5...a lot!\n",
        "print(f\"Just to have an idea...there are {total_combinations} potential combinations of hyperparameters to test.\")"
      ],
      "metadata": {
        "id": "dFVjT53jpE9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the timer\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Split into X & y\n",
        "X = hd.drop(\"target\", axis=1)\n",
        "y = hd[\"target\"]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Set n_jobs to -1 to use all available cores on your machine (if this causes errors, try n_jobs=1)\n",
        "rfc = RandomForestClassifier(n_jobs=1)  #-1 means using all processing capacity (cores)\n",
        "\n",
        "# Setup RandomizedSearchCV\n",
        "n_iter = 5 # try 5 models total\n",
        "rfc = RandomizedSearchCV(estimator=rfc,\n",
        "                            param_distributions=param_values,\n",
        "                            n_iter=n_iter,                          # how many models to try\n",
        "                            cv=5,                                   # 5-fold cross-validation\n",
        "                            verbose=2)                              # print out results\n",
        "\n",
        "# Fit the RandomizedSearchCV version of clf (does cross-validation for us, so no need to use a validation set)\n",
        "rfc.fit(X_train, y_train);\n",
        "\n",
        "# Finish the timer\n",
        "end_time = time.time()\n",
        "print(f\"[INFO] Total time taken for {n_iter} random combinations of hyperparameters: {end_time - start_time:.2f} seconds.\")"
      ],
      "metadata": {
        "id": "_yUyNyWAtli6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chech all the results and get the best combination**"
      ],
      "metadata": {
        "id": "EcALFtT3w2jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the best hyperparameters found by RandomizedSearchCV\n",
        "rfc.best_params_"
      ],
      "metadata": {
        "id": "Ys8YUZl9w2L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this process, each time we call `predict()` on `rs_clf` (our `RandomizedSearchCV` version of our classifier), it'll use the best hyperparameters it found."
      ],
      "metadata": {
        "id": "61Wwhuc7xrMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make predictions with the best hyperparameters"
      ],
      "metadata": {
        "id": "ufdlBeHx5wJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with the best hyperparameters\n",
        "rfc_y_preds = rfc.predict(X_test)\n",
        "\n",
        "# Evaluate the predictions\n",
        "rscv_metrics = evaluate_preds(y_test, rfc_y_preds)"
      ],
      "metadata": {
        "id": "AhAyrvpSx0al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exploring GridSearchCV**\n",
        "\n",
        "Since we've already tried to find some ideal hyperparameters using `RandomizedSearchCV`, we'll create another hyperparameter grid based on the `best_params_` of `rfc` with less options and then try to use `GridSearchCV` to find a more ideal set"
      ],
      "metadata": {
        "id": "MISQBdNt69M-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create hyperparameter grid similar to rfc.best_params_\n",
        "grid2 = {\"n_estimators\": [1000, 200],\n",
        "              \"max_depth\": [30,40],\n",
        "              \"max_features\": [\"log2\"],\n",
        "              \"min_samples_split\": [2, 4, 6],\n",
        "              \"min_samples_leaf\": [4]}\n",
        "grid2.values()"
      ],
      "metadata": {
        "id": "4HfpzpStAt1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2*2*1*3*1\n",
        "np.prod([len(value) for value in grid2.values()])\n",
        "# considering cv will explore 60 models!"
      ],
      "metadata": {
        "id": "qQpWnorxBcLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the timer\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Split into X & y\n",
        "X = hd.drop(\"target\", axis=1)\n",
        "y = hd[\"target\"]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Set n_jobs to -1 to use all available machine cores (if this produces errors, try n_jobs=1)\n",
        "clf = RandomForestClassifier(n_jobs=-1)\n",
        "\n",
        "# Setup GridSearchCV\n",
        "gs_clf = GridSearchCV(estimator=clf,\n",
        "                      param_grid=grid2,\n",
        "                      cv=5,         # 5-fold cross-validation\n",
        "                      verbose=2)    # print out progress\n",
        "\n",
        "# Fit the RandomizedSearchCV version of clf\n",
        "gs_clf.fit(X_train, y_train);\n",
        "\n",
        "# Find the running time\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "vn2XDsApCUy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Check the best parameters*"
      ],
      "metadata": {
        "id": "2J-XmPChxqlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the best hyperparameters found with GridSearchCV\n",
        "gs_clf.best_params_"
      ],
      "metadata": {
        "id": "-fyoZF8SA_TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Predicting*"
      ],
      "metadata": {
        "id": "gvaYFLxhDBct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Max predictions with the GridSearchCV classifier\n",
        "gs_y_preds = gs_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the predictions\n",
        "gs_metrics = evaluate_preds(y_test, gs_y_preds)\n",
        "gs_metrics"
      ],
      "metadata": {
        "id": "7irWpSFOA_RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the results better"
      ],
      "metadata": {
        "id": "DJrhKmN4DKLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\"Baseline\":bs_metrics,\n",
        "                  \"By Hand\":baseline_metrics,\n",
        "                   \"RandomSearchCV\":rscv_metrics,\n",
        "                   \"GridSearchCV\":gs_metrics})"
      ],
      "metadata": {
        "id": "bdDi-brWA_O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "NEI7dL9EA_MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Comparing performances***"
      ],
      "metadata": {
        "id": "rXDBTyjgpJA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.plot.bar(figsize=(10, 8));"
      ],
      "metadata": {
        "id": "kQrLTZDgA_Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WxI2P49g5a37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many other techniques to explore:\n",
        "\n",
        "- **Collecting more data** - Based on the results our models are getting now, it seems like they're very capable of finding patterns. Collecting more data may improve a models ability to find patterns.\n",
        "\n",
        "- Try a more advanced model - more advanced ensemble methods can be explored (XGBoost, CatBoost, etc)."
      ],
      "metadata": {
        "id": "KE4Yxkt4tPLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preserve the model\n",
        "\n",
        "The GridSearchCV model (gs_clf) looks be the better model. Lets preserve it to share or to continue working on it, later. Lets export it and save it to file.\n",
        "\n",
        "Lets serialize the model to file using the [pickle python](https://docs.python.org/3/library/pickle.html)."
      ],
      "metadata": {
        "id": "dpC7A-qJtzB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Preserve the model\n",
        "import pickle\n",
        "\n",
        "# Save an existing model to file\n",
        "fileName = \"gs_random_forest_model_1.pkl\" # .pkl extension stands for \"pickle\"\n",
        "#gs_clf was the last model\n",
        "pickle.dump(gs_clf, open(fileName, \"wb\"))"
      ],
      "metadata": {
        "id": "bXogpDt-u-3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the model"
      ],
      "metadata": {
        "id": "H-5MPWsjvs0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a saved model\n",
        "loaded_model = pickle.load(open(fileName, \"rb\"))"
      ],
      "metadata": {
        "id": "yd3TZb3evp4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the downloaded model:"
      ],
      "metadata": {
        "id": "LSR2NRC5v8yT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions and evaluate the loaded model\n",
        "new_y_preds = loaded_model.predict(X_test)\n",
        "new_model_metrics = evaluate_preds(y_test, new_y_preds)\n",
        "new_model_metrics"
      ],
      "metadata": {
        "id": "kJN0cI7itvGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\"Original GS\":gs_metrics,\n",
        "                  \"Downloade One\":new_model_metrics})\n",
        "df"
      ],
      "metadata": {
        "id": "bYAeWMcpw_9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#End!"
      ],
      "metadata": {
        "id": "RnDmi1PoA_HA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}