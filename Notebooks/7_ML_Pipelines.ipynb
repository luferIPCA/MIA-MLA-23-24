{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4uI0j_pU2U2"
      },
      "outputs": [],
      "source": [
        "#Begin!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1kf9qddTM4D"
      },
      "source": [
        "# Masters' in Applied Artificial Intelligence\n",
        "## Machine Learning Algorithms Course\n",
        "\n",
        "Notebooks for the MLA course\n",
        "\n",
        "by [*lufer*](mailto:lufer@ipca.pt)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yc6mD0jVeWN"
      },
      "source": [
        "# ML Modelling - Part VI - Pipeline\n",
        "\\\n",
        "**Contents**:\n",
        "\n",
        "1.  **Machine Learning Pipeline**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook explores the automatization of ML models creatiation, testing and improvement."
      ],
      "metadata": {
        "id": "iAEg0vfdoiFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is ML Pipeline?\n",
        "\n",
        "\n",
        "> \"(…) is a series of interconnected data processing and modelling steps designed to automate, standardize and streamline the process of building, training, evaluating and deploying machine learning models (…)\"\n",
        "(IBM)\n",
        "\n",
        "\n",
        "**Pipeline process:**\n",
        "\n",
        "1. Data retrieval and ingestion\n",
        "2. Data preparation\n",
        "3. Model training\n",
        "4. Model evaluation and tuning\n",
        "5. Model deployment\n",
        "6. Monitoring\n",
        "\n",
        "Pipeline allows to apply sequentially a list of transformers to preprocess the data and, if desired, conclude the sequence with a final predictor for predictive modeling.\n",
        "\n",
        "Intermediate steps of the pipeline must be `transforms`, that is, they must implement `fit` and `transform` methods. The final estimator only needs to implement `fit`. The transformers in the pipeline can be cached using memory argument.\n",
        "\n",
        "[see more on...](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n"
      ],
      "metadata": {
        "id": "W3Fejc7uMAHF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP-NymupVL02"
      },
      "source": [
        "## Environment preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3Rm857IVoPe"
      },
      "source": [
        "**Importing necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vA1MzNI4TU_q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "#import libraries for trainning\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "print(f\"Last updated: {datetime.datetime.now()}\")"
      ],
      "metadata": {
        "id": "wuXym8xInfIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDLxcgMwJEYA"
      },
      "source": [
        "**Mounting Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxFY0ypTJJK9"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# it will ask for your google drive credentiaals\n",
        "drive.mount('/content/gDrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1- Using a Dummy dataset\n",
        "\n",
        "Lets create a dummy classification dataset. `make_classification` alows that!\n",
        "\n",
        "\n",
        "```\n",
        "sklearn.datasets.make_classification(\n",
        "  n_samples=100,\n",
        "  n_features=20, *,\n",
        "  n_informative=2,\n",
        "  n_redundant=2,\n",
        "  n_repeated=0,\n",
        "  n_classes=2,\n",
        "  n_clusters_per_class=2,\n",
        "  weights=None,\n",
        "  flip_y=0.01,\n",
        "  class_sep=1.0,\n",
        "  hypercube=True,\n",
        "  shift=0.0,\n",
        "  scale=1.0,\n",
        "  shuffle=True,\n",
        "  random_state=None\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "This generated classification dataset has 100.000 samples and 20 features. Of the 20 features, only 2 are informative, 2 are redundant (random combinations of the informative features) and the remaining 16 are uninformative (random numbers)"
      ],
      "metadata": {
        "id": "7R3a5loaT8yR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#10000 examples (i.e., samples)\n",
        "#10 features (both of them informative features, 0 redundant)\n",
        "#1 cluster per class\n",
        "#mild class separation\n",
        "#two informative features\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=10000, n_features=10, n_informative=2, n_redundant=2, random_state=42, n_clusters_per_class=1\n",
        ")\n",
        "\n",
        "train_samples = 100  # Samples used for training the models\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    shuffle=False,\n",
        "    test_size=10000 - train_samples,\n",
        ")\n",
        "#We have data!"
      ],
      "metadata": {
        "id": "HBR-HDufUUHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check dataset insights:**"
      ],
      "metadata": {
        "id": "9jeKeVFybLZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "MKVO0GYRVIf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "ts3PWjR0VaHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplots_adjust(bottom=0.05, top=0.9, left=0.05, right=0.95)\n",
        "plt.subplot(111)\n",
        "plt.title(\"Ten features, two informative features, one clusters per class\", fontsize=\"small\")\n",
        "plt.scatter(X[:, 0], X[:, 1], marker=\"o\", c=y, s=25, edgecolor=\"k\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EHrcReLrWocJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Another example of `make_classification`**"
      ],
      "metadata": {
        "id": "X07WMbpoaoVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#just for understanding better!\n",
        "X1, y1 = make_classification(n_samples = 200\n",
        "                           ,n_features = 2\n",
        "                           ,n_informative = 2\n",
        "                           ,n_redundant = 0\n",
        "                           ,n_clusters_per_class = 1\n",
        "                           ,flip_y = 0\n",
        "                           ,class_sep = 2\n",
        "                           ,random_state = 7\n",
        "                           )\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.figure(figsize = (6,6))\n",
        "sns.scatterplot(x = X1[:,0], y = X1[:,1], hue = y1)"
      ],
      "metadata": {
        "id": "SxFy8IcmZSQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets continue with the Pipeline!"
      ],
      "metadata": {
        "id": "i4DkSWJUcJv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC     # C-Support Vector Classification model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "model = SVC()\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n",
        "# The pipeline can be used as any other estimator\n",
        "# and avoids leaking the test set into the train set\n",
        "pipe.fit(X_train, y_train)\n",
        "pipe.score(X_test, y_test)\n",
        "# An estimator's parameter can be set using '__' syntax\n",
        "# \"svc__C=10\" means set 10 to parameter \"C\" of SVC model\n",
        "# set parameter, train and evaluate\n",
        "pipe.set_params(svc__C=10).fit(X_train, y_train).score(X_test, y_test)"
      ],
      "metadata": {
        "id": "_rc18N8ITWKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Using an existing Dataset"
      ],
      "metadata": {
        "id": "LqOOL0uDgaqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 - Download Dataset"
      ],
      "metadata": {
        "id": "TddQX_cwVdRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/gDrive/MyDrive/MIA/ColabNotebooks/Datasets/heart-disease.csv\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "6qpJknTJVkRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 - Dataset insights and Preparation\n",
        "\n"
      ],
      "metadata": {
        "id": "_o-erbb4WjVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "MWqFcuHfVyWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isna().sum()\n",
        "#There are NaN values! No!"
      ],
      "metadata": {
        "id": "-fLJZIxZWFqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()\n",
        "#there is no Categorical features!"
      ],
      "metadata": {
        "id": "2uLUxxkhSXC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "FeBJnEDASfT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "PhdHXjLsghVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look about features (cor)relations:"
      ],
      "metadata": {
        "id": "_yFMGXPhhpk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pandas [crosstab](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html) function builds a cross-tabulation table that can show the frequency with which certain groups of data appear.\n"
      ],
      "metadata": {
        "id": "wRqlp5aKhh5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(data.target, data.sex)"
      ],
      "metadata": {
        "id": "773jNwYVhgtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relation between Heart Disease and the person' Sex:**"
      ],
      "metadata": {
        "id": "TjVzS6iwmM77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a plot of this crosstab\n",
        "ct=pd.crosstab(data.target, data.sex)\n",
        "ct.plot(kind=\"bar\",\n",
        "        figsize=(6, 4),\n",
        "        color=[\"lightpink\", \"lightblue\"])\n",
        "\n",
        "SMALL_SIZE = 8\n",
        "MEDIUM_SIZE = 10\n",
        "BIGGER_SIZE = 12\n",
        "\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
        "\n",
        "plt.title(\"Heart Disease Frequency for Sex\")\n",
        "plt.xlabel(\"0 = No Diesease, 1 = Disease\")\n",
        "plt.ylabel(\"Qty\")\n",
        "plt.legend([\"Female\", \"Male\"]);\n",
        "plt.xticks(rotation=0);\n"
      ],
      "metadata": {
        "id": "C6CgCnj7iumQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relation between Heart Disease and the person' Age:**"
      ],
      "metadata": {
        "id": "bxEh2XUHmnlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(data.age[data.target==1],data.thalach[data.target==1],c=\"salmon\")\n",
        "plt.scatter(data.age[data.target==0],data.thalach[data.target==0],c=\"lightblue\")\n",
        "\n",
        "plt.title(\"Distribution of Heart Disease in function of Age and Max Heart Rate\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Max Heart Rate\")\n",
        "plt.legend([\"Disease\", \"No Disease\"]);"
      ],
      "metadata": {
        "id": "7i4HRMFyj-QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relation between Heart Disease and the person' Sex:**"
      ],
      "metadata": {
        "id": "VG9MoMYymlFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.age.plot.hist();\n"
      ],
      "metadata": {
        "id": "WGhN3nZZlb6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Heart Disease Frequency per Chest Pain Type**\n",
        "\n",
        "- cp - chest pain type:\n",
        "> 0: **Typical angina**: chest pain related decrease blood supply to the heart.\n",
        ">\n",
        "> 1: **Atypical angina**: chest pain not related to heart.\n",
        ">\n",
        "> 2: **Non-anginal pain**: typically esophageal spasms (non heart related).\n",
        ">\n",
        "> 3: **Asymptomatic**: chest pain not showing signs of disease."
      ],
      "metadata": {
        "id": "2fUvC_DrmukC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ct=pd.crosstab(data.cp, data.target)\n",
        "ct"
      ],
      "metadata": {
        "id": "Byx8oT-PmvlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ct.plot(kind=\"bar\",\n",
        "        figsize=(8,4),\n",
        "        color=[\"salmon\", \"lightblue\"])\n",
        "\n",
        "# Add some communication\n",
        "plt.title(\"Heart Disease Frequency Per Chest Pain Type\")\n",
        "plt.xlabel(\"Chest Pain Type: 0 - Typical angina; 1 - Atypical angina; 2 - Non-anginal pain; 3 - Asymptomatic\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend([\"No Disease\", \"Disease\"])\n",
        "plt.xticks(rotation=0);"
      ],
      "metadata": {
        "id": "e2ccvAoUnpus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check Correlations:**"
      ],
      "metadata": {
        "id": "xo2cr5ioouZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm=data.corr()\n",
        "cm"
      ],
      "metadata": {
        "id": "lkXWN-oyon1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Graphically\n",
        "\n",
        "fig, ax= plt.subplots(figsize=(10,8))\n",
        "\n",
        "ax= sns.heatmap(cm, annot=True, linewidths=0.5, fmt=\".2f\", cmap=\"YlGnBu\")"
      ],
      "metadata": {
        "id": "71PxVsafo1Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 - Modeling"
      ],
      "metadata": {
        "id": "5rBpO4hYpqpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define X and y\n",
        "X=data.drop(\"target\", axis=1)\n",
        "y=data.target"
      ],
      "metadata": {
        "id": "-WDDomQeptis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# Split into train & test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2)"
      ],
      "metadata": {
        "id": "p7oqnUBeqCpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets explore three different models:\n",
        "- LogisticRegression\n",
        "- KNeighborsClassifier\n",
        "- RandomForestClassifier\n"
      ],
      "metadata": {
        "id": "SInGCRSVvNfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import models from Scikit-Learn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Put models in a dictionary\n",
        "models = {\"Logistic Regression\": LogisticRegression(),\n",
        "          \"KNN\": KNeighborsClassifier(),\n",
        "          \"Random Forest\": RandomForestClassifier()}\n",
        "\n",
        "# Create a function to fit and score models\n",
        "def fit_and_score(models, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Fits and evaluates given machine learning models.\n",
        "    models : a dict of differetn Scikit-Learn machine learning models\n",
        "    X_train : training data (no labels)\n",
        "    X_test : testing data (no labels)\n",
        "    y_train : training labels\n",
        "    y_test : test labels\n",
        "    \"\"\"\n",
        "    # Set random seed\n",
        "    np.random.seed(42)\n",
        "    # Make a dictionary to keep model scores\n",
        "    model_scores = {}\n",
        "    # Loop through models\n",
        "    for name, model in models.items():\n",
        "       # Fit the model to the data\n",
        "       model.fit(X_train, y_train)\n",
        "       # Evaluate the model and append its score to model_scores\n",
        "       model_scores[name] = model.score(X_test, y_test)\n",
        "    return model_scores\n",
        "\n"
      ],
      "metadata": {
        "id": "eFULCuD8qN8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#res = list(models.keys())[0]\n",
        "#analyse all models\n",
        "scores = fit_and_score(models,X_train,X_test,y_train,y_test)\n",
        "scores\n"
      ],
      "metadata": {
        "id": "NsPtmf_LqalQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 - Model Comparison"
      ],
      "metadata": {
        "id": "kkVVrSz4ucBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_compare = pd.DataFrame(scores, index=[\"accuracy\"])\n",
        "model_compare.T.plot.bar();"
      ],
      "metadata": {
        "id": "KVrZryy3ufpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 - Hyperparameter tuning\n",
        "\n",
        "**2.5.1 - by hand**"
      ],
      "metadata": {
        "id": "3x7SDTJTu6xk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore KNN model:\n",
        "\n",
        "Neighbors-based classification is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data.\n",
        "\n",
        "[See more aboout KNN in...](https://scikit-learn.org/stable/modules/neighbors.html#neighbors)"
      ],
      "metadata": {
        "id": "utjfSzUGvl9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's tune KNN\n",
        "\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "# Create a list of differnt values for n_neighbors (10 values)\n",
        "neighbors = range(1, 10)\n",
        "\n",
        "# Setup KNN instance\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Loop through different n_neighbors\n",
        "for i in neighbors:\n",
        "    knn.set_params(n_neighbors=i)\n",
        "\n",
        "    # Fit the algorithm\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # Update the training scores list\n",
        "    train_scores.append(knn.score(X_train, y_train))\n",
        "\n",
        "    # Update the test scores list\n",
        "    test_scores.append(knn.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "Y0Qv_mstu8-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the results:"
      ],
      "metadata": {
        "id": "Rz_wyUuYwRrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(neighbors, train_scores, label=\"Train score\")\n",
        "plt.plot(neighbors, test_scores, label=\"Test score\")\n",
        "plt.xticks(np.arange(1, 10, 1))\n",
        "\n",
        "plt.xlabel(\"Number of neighbors\")\n",
        "plt.ylabel(\"Model score\")\n",
        "plt.legend()\n",
        "\n",
        "print(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")"
      ],
      "metadata": {
        "id": "8q_r8mQGwTf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.5.2 - Hyperparameter tuning with RandomizedSearchCV**\n",
        "\n",
        "Lets explore the `RandomForestClassifier` model."
      ],
      "metadata": {
        "id": "MFXFvL5uwx1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Hyperparameter grid for RandomizedSearchCV\n",
        "param_values = {\"n_estimators\": [10, 100, 200, 500, 1000, 1200],\n",
        "                \"max_depth\": [None, 5, 10, 20, 30],\n",
        "                \"max_features\": [\"sqrt\", \"log2\", None],\n",
        "                \"min_samples_split\": [2, 4, 6, 8],\n",
        "                \"min_samples_leaf\": [1, 2, 4, 8]}\n",
        "\n",
        "rfc = RandomForestClassifier()\n",
        "n_iter=5\n",
        "rfc = RandomizedSearchCV(estimator=rfc,\n",
        "                            param_distributions=param_values,\n",
        "                            n_iter=n_iter,                          # how many models to try\n",
        "                            cv=5,                                   # 5-fold cross-validation\n",
        "                            verbose=2)\n",
        "# Fit the RandomizedSearchCV version of clf (does cross-validation for us, so no need to use a validation set)\n",
        "rfc.fit(X_train, y_train);"
      ],
      "metadata": {
        "id": "L0loNPxdx9Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.5.2 - Hyperparameter tuning with GridSearchCV**\n",
        "\n",
        "Lets explore the `LogisticRegression` model:"
      ],
      "metadata": {
        "id": "xFWzVAZJy_jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Different hyperparameters for our LogisticRegression model\n",
        "log_params = {\"C\": np.logspace(-4, 4, 30),\n",
        "                \"solver\": [\"liblinear\"]}\n",
        "\n",
        "# Setup grid hyperparameter search for LogisticRegression\n",
        "gs_lr = GridSearchCV(LogisticRegression(),\n",
        "                          param_grid=log_params,\n",
        "                          cv=5,\n",
        "                          verbose=True)\n",
        "\n",
        "# Fit grid hyperparameter search model\n",
        "gs_lr.fit(X_train, y_train);"
      ],
      "metadata": {
        "id": "2NX5SIDfy_SF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.5.2.1 - Check the best parameters values**"
      ],
      "metadata": {
        "id": "ZjL_uAVGz3C7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best model has the following parameters' values:"
      ],
      "metadata": {
        "id": "TJOo9hcI6BBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gs_lr.best_params_"
      ],
      "metadata": {
        "id": "8xkzN_0ZzukW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.5.2.2 - Check accuracy**"
      ],
      "metadata": {
        "id": "Jw1yX82fz-9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gs_lr.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "CPGrteg10BA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.5.2.3 - Evaluating**\n",
        "\n",
        "Evaluting our tuned machine learning classifier, beyond accuracy:\n",
        "\\\n",
        "\n",
        "- ROC curve and AUC score\n",
        "- Classification report\n",
        "- Precision\n",
        "- Recall\n",
        "- F1-score"
      ],
      "metadata": {
        "id": "5G0XwTsO6mvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predicting**"
      ],
      "metadata": {
        "id": "r_cOWlNO0HN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds_gs_lr = gs_lr.predict(X_test)\n",
        "y_preds_gs_lr"
      ],
      "metadata": {
        "id": "ORxLb5KB0JMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROC-Curve and AUC score**"
      ],
      "metadata": {
        "id": "0maEP5em0PdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we want our plots to appear inside the notebook\n",
        "%matplotlib inline\n",
        "#from sklearn.metrics import plot_roc_curve       #deprecated!!!\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "\n",
        "RocCurveDisplay.from_estimator(gs_lr, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "SQeuC-ci0Oyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.5.2.4 - Confusion Matrix**"
      ],
      "metadata": {
        "id": "q2T_d6gO2RHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "print(confusion_matrix(y_test, y_preds_gs_lr))"
      ],
      "metadata": {
        "id": "piwNVpr82Ak-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graphically"
      ],
      "metadata": {
        "id": "KS59RE8h3ZjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(font_scale=1.5)\n",
        "\n",
        "def plot_conf_mat(y_test, y_preds):\n",
        "    \"\"\"\n",
        "    Plots a nice looking confusion matrix using Seaborn's heatmap()\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(3, 3))\n",
        "    ax = sns.heatmap(confusion_matrix(y_test, y_preds_gs_lr),\n",
        "                     annot=True,\n",
        "                     cbar=False)\n",
        "    plt.xlabel(\"True label\")\n",
        "    plt.ylabel(\"Predicted label\")\n",
        "\n",
        "    bottom, top = ax.get_ylim()\n",
        "    ax.set_ylim(bottom + 0.5, top - 0.5)\n",
        "\n",
        "plot_conf_mat(y_test, y_preds_gs_lr)"
      ],
      "metadata": {
        "id": "QnKAKj3H2maK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.5.2.5 - Classification Report**"
      ],
      "metadata": {
        "id": "4ybFCqKr2T0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_preds_gs_lr))"
      ],
      "metadata": {
        "id": "NVobxRrb2O6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.5.2.6 - New instance of the \"best\" LogisticRegression model:**"
      ],
      "metadata": {
        "id": "9qTNWg0y83hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new classifier with best parameters\n",
        "clf = LogisticRegression(C=0.20433597178569418,\n",
        "                         solver=\"liblinear\")"
      ],
      "metadata": {
        "id": "qEVY_oGA0Ru3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Prediction without cross-validation*"
      ],
      "metadata": {
        "id": "xpycY1vN9QG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf.fit(X_train,y_train)\n",
        "y_preds_bestmodel = clf.predict(X_test)\n",
        "y_preds_bestmodel"
      ],
      "metadata": {
        "id": "C831lDlU9SHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_preds_bestmodel))"
      ],
      "metadata": {
        "id": "6vUJxE8y9l77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy**"
      ],
      "metadata": {
        "id": "yCgwI2iF87tR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import  cross_val_score\n",
        "# Cross-validated accuracy\n",
        "cv_acc = cross_val_score(clf,\n",
        "                         X,\n",
        "                         y,\n",
        "                         cv=5,\n",
        "                         scoring=\"accuracy\")\n",
        "cv_acc"
      ],
      "metadata": {
        "id": "IL2Gnsxm7iGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_acc = np.mean(cv_acc)\n",
        "cv_acc"
      ],
      "metadata": {
        "id": "7Vt0iXFE8I1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Precision**"
      ],
      "metadata": {
        "id": "135OW28Q8-bG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validated precision\n",
        "cv_precision = cross_val_score(clf,\n",
        "                         X,\n",
        "                         y,\n",
        "                         cv=5,\n",
        "                         scoring=\"precision\")\n",
        "cv_precision=np.mean(cv_precision)\n",
        "cv_precision"
      ],
      "metadata": {
        "id": "qjGs_dxE7_J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recall**"
      ],
      "metadata": {
        "id": "_Jf8Vm809ADM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validated recall\n",
        "cv_recall = cross_val_score(clf,\n",
        "                         X,\n",
        "                         y,\n",
        "                         cv=5,\n",
        "                         scoring=\"recall\")\n",
        "cv_recall = np.mean(cv_recall)\n",
        "cv_recall"
      ],
      "metadata": {
        "id": "l1AkrRnE8N-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**F1-score**"
      ],
      "metadata": {
        "id": "EDkaYZDJ9B-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validated f1-score\n",
        "cv_f1 = cross_val_score(clf,\n",
        "                         X,\n",
        "                         y,\n",
        "                         cv=5,\n",
        "                         scoring=\"f1\")\n",
        "cv_f1 = np.mean(cv_f1)\n",
        "cv_f1"
      ],
      "metadata": {
        "id": "gww34lpc8Tbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**See all Performance Metrics:**"
      ],
      "metadata": {
        "id": "Cw0towOv8YIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize cross-validated metrics\n",
        "cv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n",
        "                           \"Precision\": cv_precision,\n",
        "                           \"Recall\": cv_recall,\n",
        "                           \"F1\": cv_f1},\n",
        "                          index=[0])\n",
        "\n",
        "cv_metrics.T.plot.bar(title=\"Cross-validated classification metrics\",\n",
        "                      legend=False);"
      ],
      "metadata": {
        "id": "XjHLyEyC8XPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.5.2.7 - Features relevance for this LogisticRegression model**\n",
        "\n",
        "the model variable `coef` gives an array of weights estimated by linear regression, i.e., the coefficient of the features in the decision function.\n",
        "\\\n",
        "`coef_` corresponds to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False)."
      ],
      "metadata": {
        "id": "OBSvVkcW_q5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf.coef_"
      ],
      "metadata": {
        "id": "FokFGRHZAEZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "pMKMzRNBBgBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Match coef's of features to columns\n",
        "feature_dict = dict(zip(data.columns, list(clf.coef_[0])))\n",
        "feature_dict"
      ],
      "metadata": {
        "id": "1iXNu2AdBhzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize feature importance\n",
        "feature_df = pd.DataFrame(feature_dict, index=[0])\n",
        "feature_df.T.plot.bar(title=\"Feature Importance\", legend=False);"
      ],
      "metadata": {
        "id": "q5JxKngYBtKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 - Make a pipeline\n",
        "\n",
        "The pipeline is built with a list of (key, value) pairs. The key is a string containing the name you want to give and the value is the estimator object.\n",
        "\n",
        "```\n",
        "class sklearn.pipeline.Pipeline(steps, *, memory=None, verbose=False)\n",
        "```\n",
        "where `steps`are the list of **(name of step, estimator)** tuples that are to be chained in sequential order. To be compatible with the scikit-learn API, all steps must define `fit`. All non-last steps must also define `transform`.\n",
        "\n",
        "Very simple example code to show how to use was used in previous\n",
        " dummy example:\n",
        "\n",
        "```\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4Ajgb7h0eEl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 - Basic pipeline"
      ],
      "metadata": {
        "id": "J8d7iUkxGGvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets create a very basic pipeline with the following sequence:\n",
        "\n",
        "- **Scaler**: For pre-processing data, i.e., transform the data to zero mean and unit variance using the StandardScaler().\n",
        "- **Feature selector**: Use *VarianceThreshold()* for discarding features whose variance is less than a certain defined threshold.\n",
        "- **Classifier**: *KNeighborsClassifier()*, which implements the k-nearest neighbor classifier and selects the class of the majority k points, which are closest to the test example."
      ],
      "metadata": {
        "id": "2gcbzQtUDoYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_selection import VarianceThreshold # Feature selector\n",
        "\n",
        "\n",
        "pipe = Pipeline([\n",
        "#('scaler', StandardScaler()),\n",
        "#or\n",
        "('sca', StandardScaler()),          #the key name is arbitrary\n",
        "('selector', VarianceThreshold()),\n",
        "('classifier', KNeighborsClassifier())\n",
        "#('classifier', RandonForestClassifier())\n",
        "#('classifier', LogisticRegression())\n",
        "])\n",
        "#This pipe object is simple to understand. It says, scale first, select features second and classify in the end"
      ],
      "metadata": {
        "id": "1f9zCtWn_luh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pipe"
      ],
      "metadata": {
        "id": "K1UM04FmVmx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the pipe behaves like a model..thus it must be trained!\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "print('Training set score: ' + str(pipe.score(X_train,y_train)))\n",
        "print('Test set score: ' + str(pipe.score(X_test,y_test)))"
      ],
      "metadata": {
        "id": "yNQmW4wKEkCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 - Optimizing and Tuning the Pipeline\n",
        "\n",
        "Optimizing means select different solvers, parameters, etc. For instance:\n",
        "\n",
        "- Searching for other scalers. Instead of just the StandardScaler(), - we can try MinMaxScaler(), Normalizer() and MaxAbsScaler().\n",
        "- Searching for the best variance threshold to use in the selector, i.e., VarianceThreshold().\n",
        "- Searching for the best value of k for the KNeighborsClassifier()."
      ],
      "metadata": {
        "id": "VyHGlPjEF6e1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Various pre-processing steps\n",
        "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, PowerTransformer, MaxAbsScaler, LabelEncoder\n",
        "\n",
        "parameters = {'sca': [StandardScaler(), MinMaxScaler(),     #'scaler': [StandardScaler(), MinMaxScaler(),\n",
        " Normalizer(), MaxAbsScaler()],\n",
        " 'selector__threshold': [0, 0.001, 0.01],\n",
        " 'classifier__n_neighbors': [1, 3, 5, 7, 10],\n",
        " 'classifier__p': [1, 2],\n",
        " 'classifier__leaf_size': [1, 5, 10, 15]\n",
        "}\n"
      ],
      "metadata": {
        "id": "pfAHKeviGQTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters"
      ],
      "metadata": {
        "id": "Gf_I4HDyV2dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply these new parameters to the pipe \"model\""
      ],
      "metadata": {
        "id": "RSyZxuqYHlPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid = GridSearchCV(pipe, parameters, cv=2)\n",
        "grid.fit(X_train, y_train)\n",
        "#Don’t worry too much about the warning that you get by running the code above.\n",
        "#It is generated because we have very few training samples and the cross-validation object does not\n",
        "#have enough samples for a class for one of its folds."
      ],
      "metadata": {
        "id": "Hos00uUdHkab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training set score: ' + str(grid.score(X_train, y_train)))\n",
        "print('Test set score: ' + str(grid.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "Y8fL_PK9IM0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "...\n",
        "# Access the best set of parameters\n",
        "best_params = grid.best_params_\n",
        "print(best_params)\n",
        "# Stores the optimum model in best_pipe\n",
        "best_pipe = grid.best_estimator_\n",
        "print(best_pipe)"
      ],
      "metadata": {
        "id": "Rv9zWOR_N1k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to analyse the pipe result"
      ],
      "metadata": {
        "id": "xA3PQcwEOO2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#grid.cv_results_\n",
        "#Attention: to much data!!!"
      ],
      "metadata": {
        "id": "ien2tzb6OiiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = pd.DataFrame.from_dict(grid.cv_results_, orient='columns')\n",
        "print(result_df.columns)"
      ],
      "metadata": {
        "id": "IibHZKkXOSPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "SMALL_SIZE = 8\n",
        "MEDIUM_SIZE = 10\n",
        "BIGGER_SIZE = 12\n",
        "\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
        "\n",
        "\n",
        "sns.relplot(data=result_df,\n",
        " kind='line',\n",
        " x='param_classifier__n_neighbors',\n",
        " y='mean_test_score',\n",
        " hue='param_sca',\n",
        " col='param_classifier__p')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6xqUWZ7APYPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plots clearly show that using MinMaxScaler(), with n_neighbors=5 and p=1, gives the best result."
      ],
      "metadata": {
        "id": "umJbp9amQYMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 - Other Pipeline\n",
        "\n",
        "Example explored in slides"
      ],
      "metadata": {
        "id": "NG53ik3fUYSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = pd.read_csv(\"/content/gDrive/MyDrive/MIA/ColabNotebooks/Datasets/car-sales-extended-missing-data.csv\")\n",
        "ds.head()"
      ],
      "metadata": {
        "id": "TEY3zbGfnTkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset insights"
      ],
      "metadata": {
        "id": "AGS-GTaan9Aq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Data types*"
      ],
      "metadata": {
        "id": "tdr9ts0HoNjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds.dtypes"
      ],
      "metadata": {
        "id": "smS7ohJloAPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Make` and `Colour` are Categorical features!"
      ],
      "metadata": {
        "id": "6qt-Rq3CoUTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Null Values*"
      ],
      "metadata": {
        "id": "m7v1ZjikoO7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds.isna().sum()"
      ],
      "metadata": {
        "id": "OwJJSQMSoElU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several Null values in all columns!"
      ],
      "metadata": {
        "id": "yRpG7HzXocHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets work on ther dataset, trying to:\n",
        "1. Fill missing data\n",
        "2. Convert data to numbers\n",
        "3. Build a model on the data\n",
        "\n",
        "For that we'll use a pipeline!"
      ],
      "metadata": {
        "id": "wzIdpMuaorZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting data ready\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "vnsGo-hir9hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import modell and auxiliary processes\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV"
      ],
      "metadata": {
        "id": "qg4bg4fEr_Hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline for data preparation"
      ],
      "metadata": {
        "id": "EJMVUfkgsJMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup random seed\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "\n",
        "# Import data and drop the rows with missing labels\n",
        "ds.dropna(subset=[\"Price\"], inplace=True)"
      ],
      "metadata": {
        "id": "RAolwsWasO7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To impute different features with different arbitrary values, or the median, it is needed to set up several *SimpleImputer* steps within a pipeline and then join them with the *ColumnTransformer*."
      ],
      "metadata": {
        "id": "-Cm4mjYju8Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define different features and transformer pipelines\n",
        "# The SimpleImputer class provides basic strategies for imputing missing values.\n",
        "# Missing values can be imputed with a provided constant value, or using the statistics\n",
        "# (mean, median or most frequent) of each column in which the missing values are located.\n",
        "# see https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
        "# When strategy == “constant”, fill_value is used to replace all occurrences of missing_values.\n",
        "# For string or object data types, fill_value must be a string.\n",
        "# If None, fill_value will be 0 when imputing numerical data and “missing_value” for strings or object data types.\n",
        "# strategy: “mean”, “median”, “most_frequent”, or “constant”.\n",
        "\n",
        "categorical_features = [\"Make\", \"Colour\"]\n",
        "\n",
        "# this imputer imputes categorical features with \"an arbitrary value\"\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),# missing values are replaced by \"missing\"\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])                  # categorical convert in numerical\n",
        "\n"
      ],
      "metadata": {
        "id": "WOqTOHyFsoCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "door_feature = [\"Doors\"]\n",
        "# this imputer imputes categorical features with 4\n",
        "door_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=4))])"
      ],
      "metadata": {
        "id": "1PupeyC63LY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "numeric_features = [\"Odometer (KM)\"]\n",
        "# this imputer imputes numerics missing values with the mean\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"mean\"))\n",
        "])"
      ],
      "metadata": {
        "id": "frh9cZIq3N0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then we put the features list and the transformers together\n",
        "# using the \"ColumnTransformer\"\n",
        "\n",
        "# Setup preprocessing steps (fill missing values, then convert to numbers)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", categorical_transformer, categorical_features),\n",
        "        (\"door\", door_transformer, door_feature),\n",
        "        (\"num\", numeric_transformer, numeric_features)])"
      ],
      "metadata": {
        "id": "k0XXw4yr5Eys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline for model creation"
      ],
      "metadata": {
        "id": "bh_1-FeG329H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using `make_pipeline`"
      ],
      "metadata": {
        "id": "s-lhhax6KLq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example:\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "a=make_pipeline(StandardScaler(), GaussianNB(priors=None))"
      ],
      "metadata": {
        "id": "j7OgaVWCJfr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a preprocessing and modelling pipeline\n",
        "model = Pipeline(steps=[(\"preprocessor\", preprocessor),\n",
        "                        (\"model\", RandomForestRegressor(n_jobs=-1))])\n",
        "\n",
        "#equivalente a\n",
        "# model = make_pipeline(preprocessor,RandomForestRegressor(n_jobs=-1))"
      ],
      "metadata": {
        "id": "zE7iNRhS3q0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "id": "VVvW3ep4J1R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline combines a series of data preprocessing steps (filling missing values, encoding numerical values) as well as a model!"
      ],
      "metadata": {
        "id": "38Oao9ul4Zfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Or\n",
        "#numerical_imputer = SimpleImputer(strategy = \"mean\")\n",
        "#categorical_imputer = SimpleImputer(strategy=\"constant\" , fill_value = \"missing\")\n",
        "#door_imputer = SimpleImputer(strategy = \"constant\" , fill_value = 4)\n",
        "#transformer = ColumnTransformer([\n",
        "#    (\"categorical_imputer\" , categorical_imputer , categorical_features),\n",
        "#    (\"numerical_imputer\" , numerical_imputer , numeric_features),\n",
        "#    (\"door_imputer\" , door_imputer , door_feature),"
      ],
      "metadata": {
        "id": "eJKQvE1P3VF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "Dku57l0J3zsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "# Split data\n",
        "X1 = ds.drop(\"Price\", axis=1)\n",
        "y1 = ds[\"Price\"]\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2)\n",
        "\n",
        "# Fit and score the model\n",
        "model.fit(X1_train, y1_train)\n",
        "scorePipe=model.score(X1_test, y1_test)\n",
        "scorePipe"
      ],
      "metadata": {
        "id": "zCykhf8ro-9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting"
      ],
      "metadata": {
        "id": "s7LZp17x-qEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y1_preds = model.predict(X1_test)\n",
        "y1_preds.shape, y1_test.shape"
      ],
      "metadata": {
        "id": "31a4FO4f-pOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model improvement with pipeline\n",
        "\n",
        "Lets integrate `GridSearchCV` in the `Pipeline`."
      ],
      "metadata": {
        "id": "oRFLDpqw4nI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When creating a hyperparameter grid, it is necessary to add a prefix to each hyperparameter (see the [documentation for `RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) for a full list of possible hyperparameters to tune).\n",
        "\n",
        "The prefix is the name of the `Pipeline` intended to  alter, followed by two underscores.\n",
        "\n",
        "For example, to adjust `n_estimators` of `\"model\"` in the `Pipeline`, you'd use: `\"model__n_estimators\"` (note the double underscore after `model__` at the start)."
      ],
      "metadata": {
        "id": "qJ9y7ZE55h2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using grid search with pipeline\n",
        "pipe_grid = {\n",
        "    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"], # note the double underscore after each prefix \"preprocessor__\"\n",
        "    \"model__n_estimators\": [100, 1000],\n",
        "    \"model__max_depth\": [None, 5],\n",
        "    \"model__max_features\": [\"sqrt\"],\n",
        "    \"model__min_samples_split\": [2, 4]\n",
        "}\n",
        "gs_model = GridSearchCV(model, pipe_grid, cv=5, verbose=2)  #model is the pipeline!\n",
        "gs_model.fit(X1_train, y1_train)"
      ],
      "metadata": {
        "id": "rDZyFOy16Bwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Score the best model\n",
        "gsScore=gs_model.score(X1_test, y1_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "3c9W-f_l6tRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predPipe = gs_model.predict(X1_test)\n",
        "y_predPipe"
      ],
      "metadata": {
        "id": "ZOGSq-An89uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing models"
      ],
      "metadata": {
        "id": "FKOXzz8Y7UYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\"Baseline Pipe\":scorePipe,\n",
        "                   \"GridSearchCV Pipe\":gsScore}, index=[0])\n",
        "df.plot.bar(figsize=(6, 5));"
      ],
      "metadata": {
        "id": "-QeVYr5OAP-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gsPipeMetrics=evaluate_preds(y1_test, y_predPipe)"
      ],
      "metadata": {
        "id": "SSaRmIbQ8vgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing Performances"
      ],
      "metadata": {
        "id": "czIWZMHA-ZsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final = pd.DataFrame({\"GridSearchCVPipe\":gsPipeMetrics})"
      ],
      "metadata": {
        "id": "zTLi4IQe-YnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 - Example of Pipeline for Data preparation\n",
        "\n",
        "[See \"Column Transformer with Mixed Types\"](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py)"
      ],
      "metadata": {
        "id": "HbagzXfuUcAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 - Pipelines with tools\n",
        "\n",
        "* [GitHub Actions](https://neptune.ai/blog/build-mlops-pipelines-with-github-actions-guide)\n",
        "\n",
        "* [Kedro](https://neptune.ai/blog/data-science-pipelines-with-kedro)\n",
        "\n",
        "* [Metaflow](https://metaflow.org/)\n",
        "\n",
        "* [TensorFlow](https://www.tensorflow.org/?hl=pt)\n",
        "\n",
        "* Others\n",
        "\n",
        "\n",
        "Explore!"
      ],
      "metadata": {
        "id": "35rFy8P7Zkzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7 - Using ML Models in others applications"
      ],
      "metadata": {
        "id": "xqzSx_BIQT39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [Using C# to run Python Scripts with Machine Learning Models](https://ernest-bonat.medium.com/using-c-to-run-python-scripts-with-machine-learning-models-a82cff74b027)\n",
        "\n",
        "* [Using C# to call Python RESTful API Web Services with Machine Learning Models](https://ernest-bonat.medium.com/using-c-to-call-python-restful-api-web-services-with-machine-learning-models-6d1af4b7787e)\n",
        "* [Machine Learning: Models to Production](https://towardsdatascience.com/how-to-prepare-scikit-learn-models-for-production-4aeb83161bc2)"
      ],
      "metadata": {
        "id": "tPC3gB-yQdIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "\n",
        "* [Python Data Science Handbookk](https://jakevdp.github.io/PythonDataScienceHandbook/)\n",
        "\n",
        "* [Handling Missing Data with SimpleImputer](https://www.analyticsvidhya.com/blog/2022/10/handling-missing-data-with-simpleimputer/)\n",
        "\n",
        "* [How to use Sklearn to impute missing values](https://www.educative.io/answers/how-to-use-sklearn-to-impute-missing-values)\n",
        "* [Credits to...](https://www.kaggle.com/code/gunjanvermaa/prediction-model-pipeline-heart-disease/notebook)"
      ],
      "metadata": {
        "id": "QzJBjenSNljD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!End"
      ],
      "metadata": {
        "id": "rEkTLYZ9Qydx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PStohEEGQyKH"
      }
    }
  ]
}